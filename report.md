# **Technical Report: Resilient Internal Proxy Implementation**

## **1\. Introduction**

This report details the design, architecture, and implementation of an internal proxy service, developed to solve the challenge of consuming an external API with a strict rate limit of one request per second. The main objective was to engineer a resilient system capable of absorbing internal request spikes without violating the external limit, thereby minimizing penalties and ensuring a stable and observable service.

The solution was divided into two main components: a **Python (Flask) backend**, which constitutes the core of the proxy, and a **React (Vite) frontend**, which serves as an interactive dashboard for real-time control and monitoring.

## **2\. Architecture and Design Decisions**

The backend architecture was based on the classic **Producer-Consumer** pattern to ensure decoupling and efficient load management.

* **Producers (Web Server Threads):** The Flask server receives HTTP requests from clients. Each request is immediately encapsulated into a **Command** object, which contains all the necessary data for its execution. This object is then placed into a centralized queue.  
* **Buffer (In-Memory Queue):** A single priority queue (PriorityQueue) acts as a buffer. It absorbs the "bursts" of commands generated by the producers, applying backpressure when its maximum capacity is reached.  
* **Consumer (Worker Thread):** A single background **Worker** thread consumes commands from the queue at a strictly controlled pace of **one per second**. This worker is responsible for executing the command, which in turn contacts the external API.

This architecture guarantees that, regardless of the volume of incoming requests, the egress to the external service never exceeds the imposed limit.

## **3\. Adopted and Rejected Design Patterns**

The system's robustness was achieved through the application of several established design patterns.

### **3.1. Adopted Patterns**

* **Proxy:** The central pattern that defines the service's purpose as an intelligent intermediary.  
* **Command:** Used to decouple the request invoker (the API route) from the executor (the worker). This allowed us to create a queue of "jobs" rather than just data, making the system more extensible.  
* **Strategy:** Applied to allow for a flexible queueing policy. The backend always uses a PriorityQueue, but the practical behavior (FIFO or Priority) is dictated by the frontend, which decides whether to send a priority parameter.  
* **Circuit Breaker:** Implemented to increase resilience. The breaker monitors failures from the external API and "opens the circuit" to prevent cascading failures, giving the dependent service time to recover.  
* **Singleton (de facto):** The queue and the worker are instantiated only once, ensuring a centralized control point and preventing the creation of multiple rate limiters.  
* **Cache:** An in-memory cache with TTL was implemented to store recent successful responses, reducing latency and load on the external API for repeated requests.

### **3.2. Rejected Patterns**

* **Persistent Queue (e.g., Redis, RabbitMQ):** We considered using an external queue solution to ensure requests would not be lost if the service restarted. This approach was **rejected** as it added unnecessary infrastructure complexity for the scope of the challenge, which did not require persistence across restarts. The in-memory queue proved sufficient and simpler.  
* **async/await Model (asyncio):** An alternative to the threading model would be to use Python's asyncio. It was **rejected** because our specific use case (a single consumer at a slow, fixed pace) is more simply and directly modeled with a single consumer thread and a thread-safe queue.

## **4\. Analysis of Trade-offs**

* **Latency vs. Throughput:** Our architecture **prioritizes a stable and safe throughput** (1 req/s) to the external service, at the cost of **higher perceived latency** for the client during load spikes. A client may have to wait several seconds in the queue, but we guarantee that the system as a whole does not suffer penalties.  
* **User Experience (Synchronous vs. Asynchronous):** We opted for a **synchronous** interaction model, where the frontend waits for the final response. This creates a clearer user experience in our dashboard, which can display the final result of each request. The alternative, an asynchronous response (where the backend would immediately reply "request received"), would require more complex mechanisms (like WebSockets or polling) to deliver the final result to the user.  
* **Queue Fairness:** The final implementation, controlled by the frontend, offers the best of both worlds. The default FIFO behavior is "fair," but the Priority mode allows critical requests to "jump the queue," a crucial flexibility in production environments.

## **5\. Test Results and Scenarios**

The application was validated via the interactive dashboard and a test\_harness.py script. The tests confirmed that:

1. **Bursts are controlled:** A burst of 50 requests resulted in a total processing time of \~50 seconds, with the "Items in Queue" metric correctly incrementing and then decrementing at a rate of 1 per second.  
2. **Penalties are avoided:** With the rate limiter active, no 429 Too Many Requests errors were observed. When the delay was removed for testing, the error was immediately triggered, proving the rate limiter's effectiveness.  
3. **Circuit Breaker works:** After 3 consecutive requests with an invalid CPF, the breaker opened, blocking new attempts instantly. After the reset period, a successful request closed the circuit, confirming the full failure and recovery cycle.  
4. **Cache Fallback:** With the queue size artificially reduced (maxsize=2), a request for a cached CPF was served successfully even with the queue full, demonstrating the degradation strategy.

## **6\. Conclusion**

The implemented project successfully meets all the mandatory requirements of the challenge, resulting in a robust, resilient, and observable proxy service. The architectural decisions and choice of design patterns proved effective in managing load, preventing failures, and providing operational flexibility, culminating in a complete and production-ready solution.

# **Relatório Técnico: Implementação de um Proxy Interno Resiliente**

## **1\. Introdução**

Este relatório detalha a concepção, arquitetura e implementação de um serviço de proxy interno, desenvolvido para solucionar o desafio de consumir uma API externa com um limite de taxa (rate limit) estrito de uma requisição por segundo. O objetivo principal foi projetar um sistema resiliente, capaz de absorver picos de requisições internas sem violar o limite externo, minimizando penalidades e garantindo um serviço estável e observável.

O sistema foi dividido em duas componentes principais: um **backend em Python (Flask)**, que constitui o núcleo do proxy, e um **frontend em React (Vite)**, que serve como um dashboard interativo para controlo e monitorização em tempo real.

## **2\. Arquitetura e Decisões de Design**

A arquitetura do backend foi baseada no padrão clássico **Produtor-Consumidor** para garantir o desacoplamento e a gestão eficiente da carga.

* **Produtores (Web Server Threads):** O servidor Flask recebe as requisições HTTP dos clientes. Cada requisição é imediatamente encapsulada num objeto **Command**, que contém todos os dados necessários para a sua execução. Este objeto é então colocado numa fila centralizada.  
* **Buffer (Fila em Memória):** Uma única fila de prioridade (PriorityQueue) atua como um buffer. Ela absorve as "rajadas" de comandos gerados pelos produtores, aplicando *backpressure* quando a sua capacidade máxima é atingida.  
* **Consumidor (Worker Thread):** Uma única *thread*, a correr em segundo plano, consome os comandos da fila a um ritmo estritamente controlado de **um por segundo**. Este *worker* é o responsável por executar o comando, que por sua vez contacta a API externa.

Esta arquitetura garante que, independentemente do volume de requisições recebidas, a saída para o serviço externo nunca excede o limite imposto.

## **3\. Padrões de Projeto Adotados e Rejeitados**

A robustez do sistema foi alcançada através da aplicação de vários padrões de projeto consagrados.

### **3.1. Padrões Adotados**

* **Proxy:** O padrão central que define o propósito do serviço como um intermediário inteligente.  
* **Command:** Utilizado para desacoplar quem solicita a ação (a rota da API) de quem a executa (o *worker*). Isto permitiu-nos criar uma fila de "trabalhos" em vez de apenas dados, tornando o sistema mais extensível.  
* **Strategy:** Aplicado para permitir que a política de enfileiramento fosse flexível. O backend utiliza sempre uma PriorityQueue, mas o comportamento prático (FIFO ou Prioridade) é ditado pelo frontend, que decide se envia ou não um parâmetro de prioridade.  
* **Circuit Breaker (Disjuntor):** Implementado para aumentar a resiliência. O disjuntor monitoriza as falhas da API externa e "abre o circuito" para prevenir falhas em cascata, dando tempo para o serviço dependente recuperar.  
* **Singleton (de facto):** A fila e o *worker* são instanciados uma única vez, garantindo um ponto de controlo centralizado e evitando a criação de múltiplos *rate limiters*.  
* **Cache:** Um cache em memória com TTL foi implementado para armazenar respostas recentes de sucesso, reduzindo a latência e a carga na API externa para requisições repetidas.

### **3.2. Padrões Rejeitados**

* **Fila Persistente (ex: Redis, RabbitMQ):** Considerámos usar uma solução de fila externa para garantir que os pedidos não fossem perdidos se o serviço reiniciasse. Esta abordagem foi **rejeitada** por adicionar uma complexidade de infraestrutura desnecessária para o escopo do desafio, que não exigia persistência entre reinicializações. A fila em memória provou ser suficiente e mais simples.  
* \*\*Modelo \`async/await